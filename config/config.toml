# Global LLM configuration
[llm] #OLLAMA:
api_type = 'ollama'
# model = "qwen2.5:7b-instruct-fp16" # 模型需要支持工具使用
model = "qwen2.5:7b-instruct-q4_K_M" # 模型需要支持工具使用
# base_url = "http://host.docker.internal:11434/v1"
base_url = "http://localhost:11434/v1"
max_tokens = 8196
temperature = 0.0
api_key = "ollama"

# Sandbox configuration
[sandbox]
use_sandbox = false
image = "python:3.10-slim"
work_dir = "/workspace"
memory_limit = "1g"  # 512m
cpu_limit = 2.0
timeout = 300
network_enabled = false

# [llm]
# model = "claude-3-5-sonnet"
# base_url = "https://api.openai.com/v1"
# api_key = "sk-..."
# max_tokens = 4096
# temperature = 0.0

# [llm] #AZURE OPENAI:
# api_type= 'azure'
# model = "YOUR_MODEL_NAME" #"gpt-4o-mini"
# base_url = "{YOUR_AZURE_ENDPOINT.rstrip('/')}/openai/deployments/{AZURE_DEPOLYMENT_ID}"  
# api_key = "AZURE API KEY"
# max_tokens = 8096
# temperature = 0.0
# api_version="AZURE API VERSION" #"2024-08-01-preview"

# Optional configuration for specific LLM models
# [llm.vision]
# model = "claude-3-5-sonnet"
# base_url = "https://api.openai.com/v1"
# api_key = "sk-..."

[llm.vision] #OLLAMA VISION:
api_type = 'ollama'
model = "llava:7b-v1.6-mistral-q4_0" # 或你在一个名为Ollama的任何其他多模态模型中使用的 supports vision
# base_url = "http://host.docker.internal:11434/v1"
base_url = "http://localhost:11434/v1"
max_tokens = 8196
temperature = 0.0
api_key = "ollama"